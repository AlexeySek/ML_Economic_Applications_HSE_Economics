{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.linalg as spla\n",
    "import scipy.optimize as spopt\n",
    "import scipy.stats as spst\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as npla\n",
    "\n",
    "import statsmodels as sm\n",
    "import statsmodels.api as sma\n",
    "smfOLS = sma.regression.linear_model.OLS.from_formula\n",
    "smfLGT = sm.discrete.discrete_model.Logit.from_formula\n",
    "\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from inspect import signature as sig\n",
    "\n",
    "from sklearn.linear_model import Ridge as skRidge\n",
    "from sklearn.linear_model import Lasso as skLasso\n",
    "from sklearn.linear_model import ElasticNet as skEN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as skLGT\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from inspect import signature\n",
    "\n",
    "import statsmodels\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1 (1 point): Prove the equivalence of the two approaches, that is, $\\hat \\beta$ would be the same if we redefine $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the lecture we know that:\n",
    "- CS  : $\\mathcal{L}=\\prod F^{\\frac{1+y_i}{2}}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{\\frac{1-y_i}{2}} = \\prod F(y_i \\cdot \\beta' x_i /\\sigma)$ and $y_i \\in \\{-1,1\\}$\n",
    "- ECON: $\\mathcal{L}=\\prod F^{y_i}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1-y_i}$ and $y_i \\in \\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 1 - SOLUTION\n",
    "\n",
    "Proof of equivalence of two formulas\n",
    "\n",
    "##### Let's firstly assume the case when $y_i = -1$ (cs) or $y_i = 0$ (econ):\n",
    "- CS: $\\mathcal{L} = \\prod F(y_i \\cdot \\beta' x_i /\\sigma) = \\prod F(-1 \\cdot \\beta' x_i /\\sigma)$ \n",
    "- Econ: $\\mathcal{L}=\\prod F^{y_i}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1-y_i} = \\prod F^{0}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1-0} = \\prod F^{0}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1} = \\prod 1-F(\\beta' x_i /\\sigma)$\n",
    "\n",
    "From the lecture we know that $F_{log}(\\varepsilon)=\\frac{1}{1+\\exp^{-\\varepsilon}}=1-F_{log}(-\\varepsilon)$\n",
    "\n",
    "$\\Rightarrow \\prod F(-1 \\cdot \\beta' x_i /\\sigma) = \\prod 1-F(\\beta' x_i /\\sigma)$\n",
    "\n",
    "##### Then let's also check the case when $y_i = 1$ (cs) or $y_i = 1$ (econ):\n",
    "- CS: $\\mathcal{L} = \\prod F(y_i \\cdot \\beta' x_i /\\sigma) = \\prod F(1 \\cdot \\beta' x_i /\\sigma) = \\prod F(\\beta' x_i /\\sigma)$ \n",
    "- Econ: $\\mathcal{L}=\\prod F^{y_i}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1-y_i} = \\prod F^{0}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{1-0} = \\prod F^{1}(\\beta' x_i /\\sigma) \\cdot (1-F(\\beta' x_i /\\sigma))^{0} = \\prod F^{1}(\\beta' x_i /\\sigma)$\n",
    "\n",
    "Actually we got the same results $\\Rightarrow$ for $y_i = 1$ (cs) or $y_i = 1$ (econ) formulas are also equal\n",
    "\n",
    "##### So, we can conclude that if we redefine $y$, then both approaches will be equal in all the cases: if likelihood functions are equal, then optimizing them, we will get the same betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 (1 point): Find and fix the error in the code (moved the code below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the lecture we know that\n",
    "Read what regularization parameters are because they can be called differently in different packages\n",
    "- ols: $$ \\quad Loss = \\frac{1}{2n}\\sum (y_i - \\beta' x_i)^2 + a * ||w||_1 + b * \\frac{||w||^2_2}{2} \\to \\min_{\\beta}, \\quad \\text{where} \\quad l1_{ratio} = \\frac{a}{a+b}, \\quad \\alpha=a+b$$\n",
    "- logit: $$ \\quad Loss = \\sum \\log(1+\\exp(-y_i \\beta' x_i)) + a * ||w||_1 + b * \\frac{||w||^2_2}{2} \\to \\min_{\\beta}, \\quad \\text{where} \\quad l1_{ratio} = \\frac{a}{a+b}, \\quad C=a+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 2 - SOLUTION\n",
    "\n",
    "Idea is that logaddexp = log(exp(o) + exp(z)) and if we pass o = np.ones(), then we get vector of exp(1) = exp\n",
    "But according to the formula of loss (below), we need to get the vector of ones. So, we need to pass np.zeros() to the function because exp(0) = 1 and this is exactly the vector of ones that we need to get!\n",
    "$$ \\quad Loss = \\sum \\log(1+\\exp(-y_i \\beta' x_i)) + a * ||w||_1 + b * \\frac{||w||^2_2}{2} \\to \\min_{\\beta}, \\quad \\text{where} \\quad l1_{ratio} = \\frac{a}{a+b}, \\quad C=a+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code\n",
    "def OLS_loss(Y, X, beta, a, b):\n",
    "    z = Y - X@beta\n",
    "    return np.square(z).sum()/(2*Y.size) + a*np.abs(beta).sum() + b*np.square(beta).sum()/2\n",
    "\n",
    "def LGT_loss(Y, X, beta, a, b):\n",
    "    # important for #3 - here is how to calculate value inside the exponent    \n",
    "    z = -Y*X@beta # need to be not very large numbers, loagddexp needed to make computation easier\n",
    "    o = np.ones(Y.size)\n",
    "    return np.logaddexp(o, z).sum() + a*np.abs(beta).sum() + b*np.square(beta).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct code\n",
    "def OLS_loss(Y, X, beta, a, b):\n",
    "    z = Y - X@beta\n",
    "    return np.square(z).sum()/(2*Y.size) + a*np.abs(beta).sum() + b*np.square(beta).sum()/2\n",
    "\n",
    "def LGT_loss(Y, X, beta, a, b):\n",
    "    z = -Y*X@beta\n",
    "    # np.zeros instead of np.ones\n",
    "    o = np.zeros(Y.size)\n",
    "    return np.logaddexp(o, z).sum() + a*np.abs(beta).sum() + b*np.square(beta).sum()/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 (3 point): Write your own logistic regression that replicates the output of the function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 3 - SOLUTION\n",
    "\n",
    "Idea is to firstly define the loss correctly and then minimize it globally. This is actually the same as previous assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.808529</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016261</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.814209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.522363</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.155434</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x    y\n",
       "0 -0.808529  0.0\n",
       "1  0.016261  1.0\n",
       "2 -1.814209  0.0\n",
       "3  0.522363  0.0\n",
       "4 -0.155434  1.0"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset\n",
    "x=np.random.normal(size=1000)\n",
    "eps=np.random.logistic(size=1000)\n",
    "\n",
    "# careful here goes -1,1\n",
    "y = np.sign(1 + 2*x + eps)\n",
    "x = x.reshape(1000, 1)\n",
    "yecon = (y+1)/2\n",
    "\n",
    "# put columns together\n",
    "data = np.hstack((x, yecon.reshape(1000, 1)))\n",
    "data.shape\n",
    "\n",
    "# put data into dataframe\n",
    "df = pd.DataFrame(data, columns = ['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.453747\n",
      "         Iterations 7\n"
     ]
    }
   ],
   "source": [
    "# my function should give the same output as this one\n",
    "def assignment_3(formula, data):\n",
    "    model = smf.logit(formula, data=df).fit()\n",
    "    return model.params\n",
    "    \n",
    "x = assignment_3('y ~ x', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    1.017969\n",
       "x            1.781618\n",
       "dtype: float64"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGT_loss_3(Y, X, beta):\n",
    "    \n",
    "    z = -Y * X @ beta\n",
    "    o = np.zeros(Y.size)\n",
    "    \n",
    "    return np.logaddexp(o, z).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_3_answer(formula, data):\n",
    "    \n",
    "    ycolumn = formula.split(' ~ ')[0]\n",
    "    xcolumns = formula.split(' ~ ')[1].split(' + ')\n",
    "    Y = data[ycolumn].values\n",
    "    X = data[xcolumns].values\n",
    "    X = statsmodels.tools.tools.add_constant(X)\n",
    "    \n",
    "    Y = Y * 2 - 1\n",
    "    \n",
    "    Y = Y.reshape(len(Y),1)\n",
    "    \n",
    "    covs = 2\n",
    "    bounds = [(None, None)]*covs\n",
    "    betas = spopt.shgo(lambda b: LGT_loss_3(Y, X, b), bounds).x\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01796925, 1.78161848])"
      ]
     },
     "execution_count": 889,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_3_answer('y ~ x', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: я потратил 4 часа, удивляясь, почему результаты не сходятся\n",
    "\n",
    "Оказывается, в лекции была указана формула для CS, а не для Econ :))\n",
    "\n",
    "Добавив одну строку, приводящую Y в нужный вид, все получилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4 (2 points): Write your own logistic regression with $\\alpha$ and $l1ratio$ as parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 4 - SOLUTION\n",
    "\n",
    "- I simply changed the loss function from the previous assignment, adding regularization\n",
    "- Interesting thing here is that parameteres a and b were not explicitly passed to the function\n",
    "    - So, I used the formula from the seminar to derive them from this: $\\quad l1_{ratio} = \\frac{a}{a+b}, \\quad \\alpha=a+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGT_loss_4(Y, X, beta, alpha, l1ratio):\n",
    "        \n",
    "    # we can derrive a and b parameters from alpha and l1ratio\n",
    "    # alpha = a + b\n",
    "    # l1ratio = a / (a + b)\n",
    "    # a = l1ratio * (a + alpha - a) -> we put b = alpha - a instead of b here\n",
    "    \n",
    "    a = l1ratio * alpha\n",
    "    b = alpha - a\n",
    "    \n",
    "    z = -Y*X@beta\n",
    "    o = np.zeros(Y.size)\n",
    "    return np.logaddexp(o, z).sum() + a*np.abs(beta).sum() + b*np.square(beta).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_4_answer(formula, data, alpha, l1ratio):\n",
    "    \n",
    "    ycolumn = formula.split(' ~ ')[0]\n",
    "    xcolumns = formula.split(' ~ ')[1].split(' + ')\n",
    "    Y = data[ycolumn].values\n",
    "    X = data[xcolumns].values\n",
    "    X = statsmodels.tools.tools.add_constant(X)\n",
    "    Y = Y * 2 - 1\n",
    "    Y = Y.reshape(len(Y),1)\n",
    "    \n",
    "    covs = 2\n",
    "    bounds = [(None, None)]*covs\n",
    "    betas = spopt.shgo(lambda beta: LGT_loss_4(Y, X, beta, alpha, l1ratio), bounds).x\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98856345, 1.72435463])"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignment_4_answer(formula = 'y ~ x', data = df, alpha = 2, l1ratio = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5 (3 points): Write your own logistic regression with number of folds as parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 5 - SOLUTION\n",
    "\n",
    "- I took the code from the previous homework to run cross-validation to get optimal hyperparameters\n",
    "- Used loss function from the previous assignment (#4)\n",
    "- Found optimal alpha and l1ratio, put them into loss function and minimized by beta (vector of coefficients)\n",
    "- Got the estimation of beta from minimization problem, this is actually the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGT_loss_5(Y, X, beta, alpha, l1ratio):\n",
    "        \n",
    "    # we can derrive a and b parameters from alpha and l1ratio\n",
    "    # alpha = a + b\n",
    "    # l1ratio = a / (a + b)\n",
    "    # a = l1ratio / (a + alpha - a) -> we put b = alpha - a instead of b here\n",
    "    \n",
    "    a = l1ratio * alpha\n",
    "    b = alpha - a\n",
    "    \n",
    "    z = -Y*X@beta\n",
    "    o = np.zeros(Y.size)\n",
    "    \n",
    "    return np.logaddexp(o, z).sum() + a*np.abs(beta).sum() + b*np.square(beta).sum()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(alpha, l1ratio, Y, X, kf):\n",
    "    \n",
    "    avg = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        covs = 2\n",
    "        bounds = [(-20, 20)]*covs\n",
    "        betas = spopt.shgo(lambda beta: LGT_loss_5(Y_train, X_train, beta, alpha, l1ratio), bounds).x\n",
    "          \n",
    "        Y_pred = np.round(1/(1+np.exp(X@betas)))    \n",
    "        accuracy = (Y_pred == Y_test).sum() / len(Y_test)\n",
    "    \n",
    "        avg = avg + accuracy\n",
    "        \n",
    "    # get average from all our folds\n",
    "    return avg/kf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_5_answer(formula, data, folds):\n",
    "    \n",
    "    ycolumn = formula.split(' ~ ')[0]\n",
    "    xcolumns = formula.split(' ~ ')[1].split(' + ')\n",
    "    Y = data[ycolumn].values\n",
    "    X = data[xcolumns].values\n",
    "    X = statsmodels.tools.tools.add_constant(X)\n",
    "    Y = Y * 2 - 1\n",
    "    Y = Y.reshape(len(Y),1)\n",
    "\n",
    "    alphas = []\n",
    "    l1ratios = []\n",
    "    results = []\n",
    "        \n",
    "    kf = KFold(n_splits=folds)\n",
    "    \n",
    "    for alpha, l1ratio in itertools.product(np.linspace(0,2,20), np.linspace(0,2,20)):\n",
    "        results.append(run(alpha, l1ratio, Y, X, kf))\n",
    "        alphas.append(alpha)\n",
    "        l1ratios.append(l1ratio)\n",
    "    \n",
    "    opt_l1ratio = l1ratios[np.argmin(results)]\n",
    "    opt_alpha = alphas[np.argmin(results)]\n",
    "    \n",
    "    covs = 2\n",
    "    bounds = [(-20, 20)]*covs\n",
    "    betas = spopt.shgo(lambda beta: LGT_loss_5(Y, X, beta, opt_alpha, opt_l1ratio), bounds).x\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99710526, 1.74029156])"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = assignment_5_answer(formula = 'y ~ x', data = df, folds=3)\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6 (bonus 5 points): Write the ordered/latent/rand logit and compare to existing library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 6 - NO SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 7 (bonus 5 points): Write confident Logit with crossvalidation for window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 7 - NO SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
